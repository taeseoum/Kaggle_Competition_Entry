# -*- coding: utf-8 -*-
"""CS559Project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m7Gs186N-8zk3ZQ-vdlqTBVczSMkmsKk

Parametric Model: K-Fold Cross Validation
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline
import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)
import seaborn as sns
from sklearn import model_selection
from sklearn.preprocessing import OrdinalEncoder,LabelEncoder
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor

games = pd.read_csv('games.csv')
test = pd.read_csv('test.csv')
train = pd.read_csv('train.csv')
turns = pd.read_csv('turns.csv')

train['kfold'] = -1
#creat K-fold cross validation
kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)
for fold, (train_idx, val_idx) in enumerate(kf.split(X=train)):
    train.loc[val_idx, 'kfold'] = fold

print(train.kfold.value_counts())

#preprocess train data
train.isnull().sum()
test.isnull().sum()

train['nickname'] = train['nickname'].astype('category').cat.codes
test['nickname'] = test['nickname'].astype('category').cat.codes

X = train.drop(['kfold','rating'], axis=1)
y = train['rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
y_train.shape, y_test.shape, X_train.shape, X_test.shape

pred = []
scores = []

for fold in range(5):
    model = XGBRegressor(n_estimators=1000, learning_rate=0.05)
    model.fit(X_train, y_train, early_stopping_rounds=300, eval_set=[(X_test, y_test)], verbose=2000)
    predictions = model.predict(X_test)
    pred.append(predictions)
    rmse = mean_squared_error(y_test, predictions, squared=False)
    scores.append(rmse)
    print(f"fold:{fold}, rmse:{rmse}")

print(np.mean(scores))

trainPred = []
scoresTrain = []

for fold in range(5):
    model = XGBRegressor(n_estimators=1000, learning_rate=0.05)
    model.fit(X_train, y_train, early_stopping_rounds=300, eval_set=[(X_test, y_test)], verbose=2000)
    predictions = model.predict(X_train)
    trainPred.append(predictions)
    rmse = mean_squared_error(y_train, predictions, squared=False)
    scoresTrain.append(rmse)
    print(f"fold:{fold}, rmse:{rmse}")

print(np.mean(scoresTrain))

final_predictions = np.mean(np.column_stack(pred), axis=1)
while len(final_predictions) < 22363:
    final_predictions = np.append(final_predictions, np.mean(final_predictions))
sample = pd.read_csv('sample_submission.csv')
sample.rating = final_predictions[0:22364]
sample.to_csv('kfoldresults.csv', index=False)

def get_score(test,train):
  return 0.7*train + 0.3*test

#print the train rmse score
predictionsT = np.mean(trainPred, axis=0)
rmse = mean_squared_error(y_train, predictionsT, squared=False)
print(f"The Training RMSE={rmse}")
predictionR = np.mean(pred, axis=0)
rmse2 = mean_squared_error(y_test, predictionR, squared=False)
print(f"The Test RMSE={rmse2}")


print(f"The final score with 70% Training RMSE and 30% Test RMSE = {get_score(rmse,rmse2)}")

"""Non-parametric model: K-Nearest-Neighbor"""

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
games = pd.read_csv('games.csv')
test = pd.read_csv('test.csv')
train = pd.read_csv('train.csv')
turns = pd.read_csv('turns.csv')

train.isnull().sum()
test.isnull().sum()

train['nickname'] = train['nickname'].astype('category').cat.codes
test['nickname'] = test['nickname'].astype('category').cat.codes

X = train.drop(['rating'], axis=1)
y = train['rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#import knn model
from sklearn.neighbors import KNeighborsRegressor
#instantiate model
knn = KNeighborsRegressor(n_neighbors=5)
#fit model
knn.fit(X_train, y_train)
#predict
y_pred = knn.predict(X_test)
#evaluate

knn2 = KNeighborsRegressor(n_neighbors=5)
#fit model
knn2.fit(X_train, y_train)
#predict
y_predTrain = knn.predict(X_train)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(y_test, y_pred, squared=False)
print('The RMSE Test: ' ,rmse)

rmse2 = mean_squared_error(y_train, y_predTrain, squared = False)
print('The RMSE Train: ',rmse2 )



print(f"The final score with 70% Training RMSE and 30% Test RMSE = {get_score(rmse2,rmse)}")

"""Parametric Model: Linear Regression"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

games = pd.read_csv('games.csv')
test = pd.read_csv('test.csv')
train = pd.read_csv('train.csv')
turns = pd.read_csv('turns.csv')

train.head()

test.head()

games.head()

turns.head()

brief_df = pd.concat([train,test], axis=0)
brief_df = brief_df.sort_values(['game_id'])
bots     = ['BetterBot', 'STEEBot', 'HastyBot']
user_df  = brief_df[~brief_df['nickname'].isin(bots)]
user_df  = user_df.rename(columns={'nickname': 'user_name', 'score': 'user_score', 'rating': 'user_rating'})
bot_df   = brief_df[brief_df['nickname'].isin(bots)]
bot_df   = bot_df.rename(columns={'nickname': 'bot_name', 'score': 'bot_score', 'rating': 'bot_rating'})
main_df  = pd.merge(user_df, bot_df, on='game_id')
main_df.head()

main_df['user_freq'] = main_df.groupby('user_name')['user_name'].transform('count')
encode_bots          = LabelEncoder()
main_df['bot_name']  = encode_bots.fit_transform(main_df['bot_name'])
main_df.head()

train_df = main_df[~main_df['user_rating'].isna()].reset_index(drop=True)
train_df.head()

test_df  = main_df[main_df['user_rating'].isna()].reset_index(drop=True)
test_df.head()

X = train_df.drop(['user_name', 'user_rating'], axis=1)
y = train_df['user_rating'].copy()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)
model = LinearRegression()
model = model.fit(X_train, y_train)

import sklearn.metrics as metrics

reg_predict_test = model.predict(X_test)
reg_predict_train = model.predict(X_train)
train_rmse = np.sqrt(metrics.mean_squared_error(y_train, reg_predict_train))
print("Train RMSE is",train_rmse)
test_rmse = np.sqrt(metrics.mean_squared_error(y_test, reg_predict_test))
print("Test RMSE is",test_rmse)


print(f"The final score with 70% Training RMSE and 30% Test RMSE = {get_score(test_rmse,train_rmse)}")

"""**Non-parametric Model 2:**
We are gonna use Random Forest model for Non-parametric model and Data preprocessing for this model is almost same as Linear Regression model But I am gonna do data preprocessing from the start to avoid any conflicts.

Data Pre-Processing Start
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.metrics as metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.linear_model import Ridge

train = pd.read_csv('train.csv')
test  = pd.read_csv('test.csv')
turns = pd.read_csv('turns.csv')
games = pd.read_csv('games.csv')

brief_df = pd.concat([train,test], axis=0)
brief_df = brief_df.sort_values(['game_id'])
bots     = ['BetterBot', 'STEEBot', 'HastyBot']
user_df  = brief_df[~brief_df['nickname'].isin(bots)]
user_df  = user_df.rename(columns={'nickname': 'user_name', 'score': 'user_score', 'rating': 'user_rating'})
bot_df   = brief_df[brief_df['nickname'].isin(bots)]
bot_df   = bot_df.rename(columns={'nickname': 'bot_name', 'score': 'bot_score', 'rating': 'bot_rating'})
main_df  = pd.merge(user_df, bot_df, on='game_id')
main_df['user_freq'] = main_df.groupby('user_name')['user_name'].transform('count')
encode_bots          = LabelEncoder()
main_df['bot_name']  = encode_bots.fit_transform(main_df['bot_name'])
main_df.head()

train_df = main_df[~main_df['user_rating'].isna()].reset_index(drop=True)
train_df.head()

test_df  = main_df[main_df['user_rating'].isna()].reset_index(drop=True)
test_df.head()

X = train_df.drop(['user_name', 'user_rating'], axis=1)
y = train_df['user_rating'].copy()

"""Model Training"""

x_train, x_test, y_train, y_test = train_test_split(X, y, train_size= 0.8, random_state=453)

rf_model = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=8, verbose=True)

rf_model = rf_model.fit(x_train, y_train)

rf_predict_test = rf_model.predict(x_test)
rf_predict_train = rf_model.predict(x_train)

train_rmse = np.sqrt(metrics.mean_squared_error(y_train, rf_predict_train))
print("Train RMSE is",train_rmse)
test_rmse = np.sqrt(metrics.mean_squared_error(y_test, rf_predict_test))
print("Test RMSE is",test_rmse)


print(f"The final score with 70% Training RMSE and 30% Test RMSE = {get_score(test_rmse,train_rmse)}")

"""**Stack Model:**
For Stacking model we are gonna use sklearn Stacking Regressor model. Here , RidgeCV and LinearSVR models will be used as estimators and Final estimator will be RandomForestRegressor
"""

from sklearn.linear_model import RidgeCV
from sklearn.svm import LinearSVR
from sklearn.ensemble import StackingRegressor

estimators = [('lr', RidgeCV()),('svr', LinearSVR())]

reg = StackingRegressor(estimators=estimators,final_estimator=RandomForestRegressor(n_estimators=100))

reg = reg.fit(x_train, y_train)

reg_predict_test = reg.predict(x_test)
reg_predict_train = reg.predict(x_train)
train_rmse = np.sqrt(metrics.mean_squared_error(y_train, reg_predict_train))
print("Train RMSE is",train_rmse)
test_rmse = np.sqrt(metrics.mean_squared_error(y_test, reg_predict_test))
print("Test RMSE is",test_rmse)

print(f"The final score with 70% Training RMSE and 30% Test RMSE = {get_score(test_rmse,train_rmse)}")

